{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(texts, min_occurrence=5):\n",
    "    all_text = ' '.join(texts)\n",
    "    words = re.findall(r'\\b\\w+\\b', all_text.lower())  # Tokenize words using regex\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Filter out rare words\n",
    "    vocabulary = [word for word, count in word_counts.items() if count >= min_occurrence]\n",
    "\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create Reverse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_index(vocabulary):\n",
    "    reverse_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    return reverse_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Calculate Occurrence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_occurrence_probability(word, all_documents):\n",
    "    num_documents_with_word = sum(1 for doc in all_documents if word in doc)\n",
    "    total_documents = len(all_documents)\n",
    "    return num_documents_with_word / total_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculate Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conditional_probability(word, class_documents, all_documents):\n",
    "    num_class_documents_with_word = sum(1 for doc in class_documents if word in doc)\n",
    "    num_class_documents = len(class_documents)\n",
    "    return num_class_documents_with_word / num_class_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Calculate Conditional Probability with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conditional_probability_smoothed(word, class_documents, all_documents, vocabulary_size, smoothing_parameter=1):\n",
    "    num_class_documents_with_word = sum(1 for doc in class_documents if word in doc)\n",
    "    num_class_documents = len(class_documents)\n",
    "    return (num_class_documents_with_word + smoothing_parameter) / (num_class_documents + smoothing_parameter * vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Predict Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(document, vocabulary, occurrence_probs_human, occurrence_probs_llm, conditional_probs_human, conditional_probs_llm):\n",
    "    words = re.findall(r'\\b\\w+\\b', document.lower())\n",
    "    log_prob_human = math.log(occurrence_probs_human)\n",
    "    log_prob_llm = math.log(occurrence_probs_llm)\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            log_prob_human += math.log(conditional_probs_human[vocabulary[word]])\n",
    "            log_prob_llm += math.log(conditional_probs_llm[vocabulary[word]])\n",
    "\n",
    "    return \"human\" if log_prob_human > log_prob_llm else \"llm\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(dev_documents, dev_labels, vocabulary, occurrence_probs_human, occurrence_probs_llm, conditional_probs_human, conditional_probs_llm):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for doc, label in zip(dev_documents, dev_labels):\n",
    "        predicted_class = predict_class(doc, vocabulary, occurrence_probs_human, occurrence_probs_llm, conditional_probs_human, conditional_probs_llm)\n",
    "        if predicted_class == label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(dev_documents)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Compare the Effect of Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Derive Top 10 Words Predicting Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_for_class(class_probs, vocabulary, top_n=10):\n",
    "    sorted_words = sorted(vocabulary, key=lambda word: class_probs[word], reverse=True)\n",
    "    return sorted_words[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# (Assuming you have separate lists for human and LLM essays in the training and development sets)\n",
    "human_train_essays = [...]  \n",
    "llm_train_essays = [...]\n",
    "\n",
    "human_dev_essays = [...]\n",
    "llm_dev_essays = [...]\n",
    "\n",
    "# Assuming you have already created the vocabulary, reverse index, and split the data\n",
    "vocabulary = build_vocabulary(human_train_essays + llm_train_essays)\n",
    "\n",
    "occurrence_probs_human = calculate_occurrence_probability(\"the\", human_train_essays)\n",
    "occurrence_probs_llm = calculate_occurrence_probability(\"the\", llm_train_essays)\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "conditional_probs_human = {word: calculate_conditional_probability(word, human_train_essays, human_train_essays) for word in vocabulary}\n",
    "conditional_probs_llm = {word: calculate_conditional_probability(word, llm_train_essays, llm_train_essays) for word in vocabulary}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
